{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145 entries, 0 to 144\n",
      "Data columns (total 3 columns):\n",
      "TrainingUtterance    145 non-null object\n",
      "TrainingAnswer       145 non-null object\n",
      "TestUtterance        145 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.5+ KB\n"
     ]
    }
   ],
   "source": [
    "cq = pd.read_csv('test-utterances_clean.csv', header=0, encoding='latin1')\n",
    "cq.drop('Pass/ Fail', axis=1, inplace=True)\n",
    "cq.columns = [\"TrainingUtterance\", \"TrainingAnswer\", \"TestUtterance\"]\n",
    "cq.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = {\"Question\" : [], \"Answer\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"All Questions.txt\", 'r', encoding='latin1') as f:\n",
    "    for line in f.readlines():\n",
    "        if line.lower().startswith('q:'):\n",
    "            ques = line.replace('Q:', '').replace('\\n', '').lstrip()\n",
    "            train_corpus[\"Question\"].append(ques)\n",
    "        if line.lower().startswith('a:'):\n",
    "            ans = line.replace('A:', '').replace('\\n', '').lstrip()\n",
    "            train_corpus[\"Answer\"].append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "661"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corpus[\"Question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 661 entries, 0 to 660\n",
      "Data columns (total 3 columns):\n",
      "Answer      661 non-null object\n",
      "Question    661 non-null object\n",
      "qna         661 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 15.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame.from_dict(train_corpus)\n",
    "data['qna'] = data[data.columns[[0,1]]].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    try:\n",
    "        tokens_ = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "        \n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        tokens = list(filter(lambda t: t not in punctuation, tokens))\n",
    "        tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\", u\"...\", u\"''\", u'``', \n",
    "                                            u'\\u2014', u'\\u2026', u'\\u2013'], tokens))\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "\n",
    "        filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n",
    "\n",
    "        return filtered_tokens\n",
    "    except Error as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read synonyms file into a dictionary.\n",
    "synonyms = {}\n",
    "with open(\"di_synonyms.csv\", 'r', encoding='latin1') as sf:\n",
    "    for row in sf.readlines():\n",
    "        row = row.replace(\"\\n\", '').split(\",\")\n",
    "        canonical, rest = row[0], row[1:]\n",
    "        for term in rest:\n",
    "            if term:\n",
    "                synonyms[term.lower()] = canonical.lower()\n",
    "                synonyms[canonical.lower()] = canonical.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def syn_words(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for seq in ngrams(tokens, 3):\n",
    "        rep_str = ' '.join(seq)\n",
    "        if rep_str.lower() in synonyms:\n",
    "            sentence = sentence.replace(rep_str, synonyms[rep_str.lower()])\n",
    "    for seq in ngrams(tokens, 2):\n",
    "        rep_str = ' '.join(seq)\n",
    "        if rep_str.lower() in synonyms:\n",
    "            sentence = sentence.replace(rep_str, synonyms[rep_str.lower()])\n",
    "    for seq in ngrams(tokens, 1):\n",
    "        rep_str = ' '.join(seq)\n",
    "        if rep_str.lower() in synonyms:  \n",
    "            sentence = sentence.replace(rep_str, synonyms[rep_str.lower()])    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cq['CleanedTestUtterance'] = cq['TestUtterance'].map(syn_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  0,   1,   2,   4,   5,   7,   8,   9,  11,  12,  16,  18,  20,\n",
       "             21,  26,  28,  29,  32,  34,  35,  37,  38,  39,  42,  43,  45,\n",
       "             46,  50,  53,  59,  61,  67,  68,  69,  71,  73,  74,  75,  81,\n",
       "             84,  86,  89,  90,  92,  93,  96, 103, 104, 105, 106, 107, 110,\n",
       "            111, 114, 115, 117, 118, 119, 121, 123, 125, 127, 128, 131, 134,\n",
       "            135, 136, 137, 141],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_ids = cq[cq['CleanedTestUtterance'] != cq['TestUtterance']].index\n",
    "row_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['CleanedQuestion'] = data['Question'].map(syn_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapped_ans_with_confidence(tq_idx):\n",
    "    matched_idx = best_match[tq_idx].argmax()\n",
    "    matched_confidence = best_match[tq_idx][matched_idx]\n",
    "    matched_ques = data['Question'][matched_idx]\n",
    "    return matched_ques\n",
    "\n",
    "def top_n_matches(tq_idx, n=3):\n",
    "    matched_indices = best_match[tq_idx].argsort()[-n:]\n",
    "    return [data['Question'][qidx] for qidx in matched_indices] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = ['qna', 'CleanedQuestion']\n",
    "vsm1 = ['qna', 'CleanedQuestion']\n",
    "vsm2 = ['CleanedTestUtterance']\n",
    "min_df_options = list(range(1,11,1))\n",
    "max_features = list(range(1000, 11000, 1000))\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, max_features=10000, tokenizer=tokenizer, ngram_range=(1, 2))\n",
    "for col in vocab:\n",
    "    vocab_vec = vectorizer.fit_transform(list(data[col]))\n",
    "    for v1 in vsm1:\n",
    "        vec1 = vectorizer.transform(list(data[v1]))\n",
    "        for v2 in vsm2:\n",
    "            vec2 = vectorizer.transform(list(cq[v2]))\n",
    "            best_match = cosine_similarity(vec2, vec1)\n",
    "            \n",
    "            res_matrix = [mapped_ans_with_confidence(i) for i in range(0, len(cq))]\n",
    "            cq['matched_ques'] = res_matrix\n",
    "            \n",
    "            topn_matrix = [top_n_matches(i) for i in range(0, len(cq))]\n",
    "            cq['topn'] = topn_matrix\n",
    "            \n",
    "            cq['IsBestMatch'] = np.where(cq['TrainingUtterance'] == cq['matched_ques'], 1, 0)\n",
    "            cq['IsTopNMatch'] = cq.apply(lambda x : 1 if x['TrainingUtterance'] in x['topn'] else 0, axis=1)\n",
    "            \n",
    "            best_topn_accuracy = (cq['IsTopNMatch']==1).sum() / (len(cq)-1)\n",
    "            best_accuracy =      (cq['IsBestMatch']==1).sum() / (len(cq)-1)\n",
    "            results.append({\n",
    "                \"vocab\" : col,\n",
    "                \"vsm1\"  : v1,\n",
    "                \"vsm2\"  : v2,\n",
    "                \"BestAccuracy\" : round(best_accuracy*100,2),\n",
    "                \"TopNAccuracy\" : round(best_topn_accuracy*100,2)\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = sorted(results, key=lambda k: k['TopNAccuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ListTable(list):  \n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            \n",
    "            for col in row:\n",
    "                html.append(\"<td>{0}</td>\".format(col))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><b>Vocabulary</b></td><td><b>Target Text </b></td><td><b>Top N Match Percentage</b></td><td><b>Best Match Percentage</b></td><td><b>Source Text</b></td></tr><tr><td>CleanedQuestion</td><td>qna</td><td>54.86</td><td>34.03</td><td>CleanedTestUtterance</td></tr><tr><td>qna</td><td>qna</td><td>61.11</td><td>38.19</td><td>CleanedTestUtterance</td></tr><tr><td>CleanedQuestion</td><td>CleanedQuestion</td><td>61.11</td><td>47.22</td><td>CleanedTestUtterance</td></tr><tr><td>qna</td><td>CleanedQuestion</td><td>69.44</td><td>50.0</td><td>CleanedTestUtterance</td></tr></table>"
      ],
      "text/plain": [
       "[['<b>Vocabulary</b>',\n",
       "  '<b>Target Text </b>',\n",
       "  '<b>Top N Match Percentage</b>',\n",
       "  '<b>Best Match Percentage</b>',\n",
       "  '<b>Source Text</b>'],\n",
       " dict_values(['CleanedQuestion', 'qna', 54.859999999999999, 34.030000000000001, 'CleanedTestUtterance']),\n",
       " dict_values(['qna', 'qna', 61.109999999999999, 38.189999999999998, 'CleanedTestUtterance']),\n",
       " dict_values(['CleanedQuestion', 'CleanedQuestion', 61.109999999999999, 47.219999999999999, 'CleanedTestUtterance']),\n",
       " dict_values(['qna', 'CleanedQuestion', 69.439999999999998, 50.0, 'CleanedTestUtterance'])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = ListTable()\n",
    "table.append(['<b>Vocabulary</b>', '<b>Target Text </b>', '<b>Top N Match Percentage</b>', '<b>Best Match Percentage</b>', '<b>Source Text</b>'])\n",
    "for d in results:\n",
    "    table.append(d.values())\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the above experiment we see that training the bot on question-and-answer pair as a single document gives the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145 entries, 0 to 144\n",
      "Data columns (total 5 columns):\n",
      "TrainingUtterance        145 non-null object\n",
      "TrainingAnswer           145 non-null object\n",
      "TestUtterance            145 non-null object\n",
      "Persistent Bot Result    90 non-null object\n",
      "AvadhootComment          46 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 5.7+ KB\n"
     ]
    }
   ],
   "source": [
    "review_df = pd.read_csv('avadhoot_analysis.csv', header=0, encoding='latin1')\n",
    "review_df.drop(' Pass/Fail', axis=1, inplace=True)\n",
    "review_df.columns = [\"TrainingUtterance\", \"TrainingAnswer\", \"TestUtterance\", 'Persistent Bot Result','AvadhootComment']\n",
    "review_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = TfidfVectorizer(min_df=2, max_features=10000, tokenizer=tokenizer, ngram_range=(1, 2))\n",
    "model.fit(list(data['qna']))\n",
    "v1 = model.transform(list(cq['CleanedTestUtterance']))\n",
    "v2 = model.transform(list(data['CleanedQuestion']))\n",
    "cosine_distances = cosine_similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oss_answers = [ data.Question[row.argmax()] for row in cosine_distances]\n",
    "review_df['OSS_ANS'] = oss_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_n_matches_model(tq_idx, n=3):\n",
    "    matched_indices = cosine_distances[tq_idx].argsort()[-n:]\n",
    "    return [data['Question'][qidx] for qidx in matched_indices] \n",
    "\n",
    "topn_matrix = [top_n_matches_model(i) for i in range(0, len(review_df))]\n",
    "review_df['topn'] = topn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_df = review_df[~review_df['AvadhootComment'].isin(['Enhancement: greeting handling', 'Bug'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestAccuracy 51.09\n"
     ]
    }
   ],
   "source": [
    "review_df['IsBestMatch'] = np.where(review_df['TrainingUtterance'] == review_df['OSS_ANS'], 1, 0)\n",
    "best_accuracy = (review_df['IsBestMatch']==1).sum() / (len(review_df)-1)\n",
    "print(\"BestAccuracy {}\".format(round(best_accuracy*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestTopNAccuracy 70.07\n"
     ]
    }
   ],
   "source": [
    "review_df['IsTopNMatch'] = review_df.apply(lambda x : 1 if x['TrainingUtterance'] in x['topn'] else 0, axis=1)\n",
    "best_topn_accuracy = (review_df['IsTopNMatch']==1).sum() / (len(review_df)-1)\n",
    "print(\"BestTopNAccuracy {}\".format(round(best_topn_accuracy*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 138 entries, 0 to 144\n",
      "Data columns (total 9 columns):\n",
      "TrainingUtterance        138 non-null object\n",
      "TrainingAnswer           138 non-null object\n",
      "TestUtterance            138 non-null object\n",
      "Persistent Bot Result    89 non-null object\n",
      "AvadhootComment          39 non-null object\n",
      "OSS_ANS                  138 non-null object\n",
      "topn                     138 non-null object\n",
      "IsBestMatch              138 non-null int32\n",
      "IsTopNMatch              138 non-null int64\n",
      "dtypes: int32(1), int64(1), object(7)\n",
      "memory usage: 10.2+ KB\n"
     ]
    }
   ],
   "source": [
    "review_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_df.drop(['TrainingAnswer', 'Persistent Bot Result'], inplace=True, axis=1)\n",
    "review_df.to_csv(\"DI_bot_results.tsv\", sep='\\t', quoting=csv.QUOTE_NONE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
